{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "[ You can use both the raw EEG data and EEG smooth feature data and plot both to understand them and see what to classify. ]\n",
    "\n",
    "First, start with one or two participants (possibly someone who has x and someone who doesn't have x) to figure out some kind of pattern. In the end you would take all the information and clump the participants into categories (if they are all in clumps you can take the average and see what the predominant feature is in them)\n",
    "\n",
    "~Possible helpful tool to split data into training and testing set is sk.learn, which is a training test split. It splits true or false values so you can epoch classifiers. \n",
    "\n",
    "\n",
    "Understanding the feature smooth data: \n",
    "- it says each field is in the shape of channel_number (62) x sample number (W; W indicates the number of time windows in that trial (different trials have different W because the film clips are not of the same length and each time window is 4 seconds) x frequency_bands (5): 1) delta: 1~4 Hz; 2) theta: 4~8 Hz; 3) alpha: 8~14 Hz; 4) beta: 14~31 Hz; and 5) gamma: 31~50 Hz). \n",
    "- this means the data is 3 dimensional \n",
    "    - the first would be one channel (ex like one electrode)\n",
    "    - the second is a time point (ex at 2000 ms)\n",
    "    - the third dimension is a frequency band (0 = delta, 1 = theta, 2 = alpha, 3 = beta, and 4 = gamma)\n",
    "    \n",
    "    \n",
    "~possible classifiers:\n",
    "- might be best to use SVM for feature smooth data because it works well with high dimensional data \n",
    "- LDA might be good to use for raw data and a lot of articles say it is the most popular classifier (maybe if you find problems with the results it could be an interesting point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars\n",
    "NUM_SUBJECTS = 15\n",
    "SESSION1_LABELS = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3]\n",
    "ESI_Neuroscan_System_channels = ['FP1', 'FPZ', 'FP2', 'AF3', 'AF4', 'F7', 'F5', 'F3', 'F1', 'FZ', \n",
    "                                 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', \n",
    "                                 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1','CZ', 'C2', 'C4', 'C6', \n",
    "                                 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8', \n",
    "                                 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO5', \n",
    "                                 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', 'CB1', 'O1', 'OZ', 'O2', 'CB2']\n",
    "ten_twenty_international_system_channels = ['FP1', 'FP2', 'F7', 'F3', 'FZ', 'F4', 'F8', 'T3', 'C3', \n",
    "                                            'CZ', 'C4', 'T4', 'T5', 'P3', 'PZ', 'P4', 'T6', 'O1', 'O2']\n",
    "frequency_bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "DATA_COL_NAMES = [f\"{channel}_{freq_band}\" for channel in ESI_Neuroscan_System_channels for freq_band in frequency_bands] + [\"emotionLabel\"]\n",
    "CHANNELS_INDEX = 0; WINDOWS_INDEX = 1; FREQUENCY_BANDS_INDEX = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_dir = \"SEED_IV/eeg_feature_smooth/1\"\n",
    "session1_subjects_data = [None] * NUM_SUBJECTS\n",
    "print(\"Loading data from session 1:\")\n",
    "for filename in os.listdir(session1_dir):\n",
    "    f = os.path.join(session1_dir, filename)\n",
    "    subject, date = filename.split(\"_\")\n",
    "    print(f\"\\tLoading data from subject {subject}...\")\n",
    "    session1_subjects_data[int(subject)-1] = loadmat(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to parse the data into 4 dataframes: <br>\n",
    "- **session1_de_movingAve**\n",
    "- **session1_de_lds**\n",
    "- **session1_psd_movingAve**\n",
    "- **session1_psd_lds**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_title(data_title: str) -> Tuple[str, str, str]:\n",
    "    '''\n",
    "    Parse the key in the data dictionary into feature, smoothing method, and trial number.\n",
    "    '''\n",
    "    if data_title in [\"__globals__\", \"__header__\", \"__version__\"]:\n",
    "        return \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "    # 2-digit trial number\n",
    "    if data_title[-2].isdigit():\n",
    "        feature, smoothing_method_and_trial_num = data_title.split(\"_\")\n",
    "        return feature, smoothing_method_and_trial_num[:-2], smoothing_method_and_trial_num[-2:]\n",
    "\n",
    "    # 1-digit trial number\n",
    "    else:\n",
    "        feature, smoothing_method_and_trial_num = data_title.split(\"_\")\n",
    "        return feature, smoothing_method_and_trial_num[:-1], smoothing_method_and_trial_num[-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_trial(data: np.ndarray, trial_num: int):\n",
    "       '''\n",
    "       Parse data from one trial into a dataframe and return it.\n",
    "       '''\n",
    "       trial_df = pd.DataFrame(columns=DATA_COL_NAMES)\n",
    "       emotionLabel = SESSION1_LABELS[trial_num-1]\n",
    "       # add windows in the trial as samples to the trial df\n",
    "       for window in range(data.shape[WINDOWS_INDEX]):\n",
    "              window_mat = np.array(data[:, window, :])\n",
    "              row = np.append(window_mat.flatten(), emotionLabel).reshape(1, len(DATA_COL_NAMES))\n",
    "              row_df = pd.DataFrame(row, columns=DATA_COL_NAMES)\n",
    "              trial_df = pd.concat([trial_df, row_df])\n",
    "\n",
    "       return trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframes\n",
    "session1_de_movingAve = pd.DataFrame(columns=DATA_COL_NAMES)\n",
    "session1_de_lds = pd.DataFrame(columns=DATA_COL_NAMES)\n",
    "session1_psd_movingAve = pd.DataFrame(columns=DATA_COL_NAMES)\n",
    "session1_psd_lds = pd.DataFrame(columns=DATA_COL_NAMES)\n",
    "\n",
    "# loop over all 15 subjects\n",
    "for subject_num, subject_data in enumerate(session1_subjects_data):\n",
    "    print(f\"Adding data from subject {subject_num+1}...\")\n",
    "    # loop over all the different features, smoothing methods, and trials\n",
    "    for data_title in subject_data:\n",
    "        # parse the data into row to add to the dataframe\n",
    "        feature, smoothing_method, trial = parse_data_title(data_title)\n",
    "        #print(f\"parsing feature: {feature} | smoothing method: {smoothing_method} | trial: {trial}\")\n",
    "        \n",
    "        # a field that doesn't need to be parsed\n",
    "        if feature == \"N/A\":\n",
    "            continue\n",
    "\n",
    "        #which dataframe does it go into?\n",
    "        if feature == \"de\" and smoothing_method == 'movingAve':\n",
    "            # add to session1_de_movingAve dataframe\n",
    "            session1_de_movingAve =  pd.concat([session1_de_movingAve, parse_trial(subject_data[data_title], int(trial))])\n",
    "\n",
    "        if feature == \"de\" and smoothing_method == 'LDS':\n",
    "            # add to session1_de_movingAve dataframe\n",
    "            session1_de_lds =  pd.concat([session1_de_lds, parse_trial(subject_data[data_title], int(trial))])\n",
    "\n",
    "        if feature == \"psd\" and smoothing_method == 'movingAve':\n",
    "            # add to session1_de_movingAve dataframe\n",
    "            session1_psd_movingAve =  pd.concat([session1_psd_movingAve, parse_trial(subject_data[data_title], int(trial))])\n",
    "\n",
    "        if feature == \"psd\" and smoothing_method == 'LDS':\n",
    "            # add to session1_de_movingAve dataframe\n",
    "            session1_psd_lds =  pd.concat([session1_psd_lds, parse_trial(subject_data[data_title], int(trial))])\n",
    "\n",
    "session1_de_movingAve = session1_de_movingAve.reset_index(drop=True)\n",
    "session1_de_lds = session1_de_lds.reset_index(drop=True)\n",
    "session1_psd_movingAve = session1_de_movingAve.reset_index(drop=True)\n",
    "session1_psd_lds = session1_psd_lds.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_de_movingAve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_de_lds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_psd_movingAve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_psd_lds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce channels to only 10/20 international system channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ten_twenty_international_system_channel(col_name: str) -> bool:\n",
    "    '''\n",
    "    Return True if a column corresponds to a 10/20 international system channel (and not just an ESI channel), \n",
    "    False otherwise.\n",
    "    '''\n",
    "\n",
    "    if col_name == 'emotionLabel':\n",
    "        return True\n",
    "\n",
    "    esi_channel, freq_band = col_name.split(\"_\")\n",
    "    #print(f\"{esi_channel} matched these 10/20 is channels: {np.array(ten_twenty_international_system_channels)[arr]}\")\n",
    "    return any(is_channel == esi_channel for is_channel in ten_twenty_international_system_channels)\n",
    "\n",
    "assert is_ten_twenty_international_system_channel(\"FP1_gamma\"), \"FP1_gamma unit test failed\"\n",
    "assert is_ten_twenty_international_system_channel(\"O2_beta\"), \"O2_beta unittest failed\"\n",
    "assert not is_ten_twenty_international_system_channel(\"CP4_alpha\"), \"CP4_alpha unit test failed\"\n",
    "assert not is_ten_twenty_international_system_channel(\"TP7_theta\"), \"TP7_theta unit test failed\"\n",
    "assert is_ten_twenty_international_system_channel(\"emotionLabel\"), \"emotionLabel unit test failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_channels_cols = [col for col in DATA_COL_NAMES if is_ten_twenty_international_system_channel(col)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! Some 10/20 interational system channels are not ESI channels as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_channels = set(map(lambda s: s.split(\"_\")[0], reduced_channels_cols))\n",
    "print(f\"These 10/20 international system channels are missing: {set(ten_twenty_international_system_channels) - reduced_channels}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, T7, T8, P7, and P8 are pretty close to those channels so we can just include them as well to recover that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_extra_channel(col_name: str) -> bool:\n",
    "    '''\n",
    "    Return True if a column corresponds to one of the extra channels, False otherwise.\n",
    "    '''\n",
    "    esi_channel, freq_band = col_name.split(\"_\")\n",
    "    return any(esi_channel == extra_channel for extra_channel in [\"T7\", \"T8\", \"P7\", \"P8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [col for col in DATA_COL_NAMES if is_ten_twenty_international_system_channel(col) or is_extra_channel(col)]\n",
    "\n",
    "session1_de_movingAve = session1_de_movingAve[cols_to_keep]\n",
    "session1_de_lds = session1_de_lds[cols_to_keep]\n",
    "session1_psd_movingAve = session1_psd_movingAve[cols_to_keep]\n",
    "session1_psd_lds = session1_psd_lds[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_de_movingAve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_de_lds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_psd_movingAve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1_psd_lds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Now we have the 4 dataframes (session1_de_movingAve, session1_de_lds, session1_psd_movingAve, session1_psd_lds) representing the 4 different possible feature/smoothing methods matchups.\n",
    "\n",
    "Now, the meat of our project. We want to see which combination of feature, smoothing method, and classifier is the best at predicting what type of film clip the participant was watching: (DE (differential entropy), PSD (power spectral density)) x (movingAve, LDS (linear dynamic system)) x (SVC(rbf kernel), random forest, k-nearest neighbors). Let's just assume we use classification accuracy as our metric to measure which is the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample datasets for testing\n",
    "\n",
    "SAMPLE_FRAC = 0.01\n",
    "sample_session1_de_movingAve = session1_de_movingAve.sample(frac=SAMPLE_FRAC)\n",
    "sample_session1_de_lds = session1_de_lds.sample(frac=SAMPLE_FRAC)\n",
    "sample_session1_psd_movingAve = session1_psd_movingAve.sample(frac=SAMPLE_FRAC)\n",
    "sample_session1_de_lds = session1_de_lds.sample(frac=SAMPLE_FRAC)\n",
    "\n",
    "sample_datasets = [(sample_session1_de_movingAve, \"de\", \"movingAve\"), (sample_session1_de_lds, \"de\", \"LDS\"), \n",
    "                   (sample_session1_psd_movingAve, \"psd\", \"movingAve\"), (sample_session1_de_lds, \"de\", \"LDS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_param_search_space = {'C': np.logspace(-3, 3, 7), \n",
    "                    'gamma': ['scale', 'auto'], \n",
    "                    'kernel': ['rbf']}\n",
    "\n",
    "rf_param_search_space = {'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                         'criterion': ['gini', 'entropy']}\n",
    "\n",
    "knn_param_search_space = {'n_neighbors': list(map(int, np.linspace(5, sqrt(len(session1_de_movingAve)), num=5))), \n",
    "                          'weights': ['uniform', 'distance']}\n",
    "\n",
    "sample_knn_param_search_space = {'n_neighbors': list(map(int, np.linspace(5, sqrt(len(sample_session1_de_movingAve)), num=5))), \n",
    "                          'weights': ['uniform', 'distance']}\n",
    "NUM_TRIALS = 10\n",
    "\n",
    "MAX_ITER = 10000000\n",
    "classifiers = [(SVC(max_iter=MAX_ITER), svc_param_search_space, \"SVC\"), \n",
    "               (RandomForestClassifier(), rf_param_search_space, \"Random Forest\"), \n",
    "               (KNeighborsClassifier(n_neighbors=5), knn_param_search_space, \"K-Nearest Neighbors\")]\n",
    "\n",
    "sample_classifiers = [(SVC(max_iter=MAX_ITER), svc_param_search_space, \"SVC\"), \n",
    "               (RandomForestClassifier(), rf_param_search_space, \"Random Forest\"), \n",
    "               (KNeighborsClassifier(n_neighbors=5), sample_knn_param_search_space, \"K-Nearest Neighbors\")]\n",
    "\n",
    "datasets = [(session1_de_movingAve, \"de\", \"movingAve\"), (session1_de_lds, \"de\", \"LDS\"), \n",
    "            (session1_psd_movingAve, \"psd\", \"movingAve\"), (session1_psd_lds, \"psd\", \"LDS\")]\n",
    "\n",
    "ANALYSIS_RESULTS_DF_COL_NAMES = ['feature', 'smoothing method', 'classifier', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run with sample datasets to first make sure the code works\n",
    "analysis_results_df = pd.DataFrame(columns=ANALYSIS_RESULTS_DF_COL_NAMES)\n",
    "print(f\"Running Analysis...\\n\")\n",
    "print(f\"FEATURE\\tSMOOTHING METHOD\\tCLASSIFIER\")\n",
    "for trial in range(NUM_TRIALS):\n",
    "    for clf, clf_param_search_space, clf_name in sample_classifiers:\n",
    "        for dataset, feature, smoothing_method in sample_datasets:\n",
    "            print(f\"{feature}\\t{smoothing_method}\\t\\t{clf_name}\")\n",
    "\n",
    "            # train model\n",
    "            X = dataset.drop('emotionLabel', axis=1); y = dataset.emotionLabel\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "            grid = GridSearchCV(clf, clf_param_search_space, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", \n",
    "                                refit=True).fit(X_train, y_train)\n",
    "            \n",
    "            # record results in dataframe\n",
    "            test_set_accuracy_score = grid.score(X_test, y_test)\n",
    "            row = [[feature, smoothing_method, clf_name, test_set_accuracy_score]]\n",
    "            analysis_results_df = pd.concat([analysis_results_df, pd.DataFrame(row, columns=ANALYSIS_RESULTS_DF_COL_NAMES)])\n",
    "\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now let's run the real analysis\n",
    "analysis_results_df = pd.DataFrame(columns=ANALYSIS_RESULTS_DF_COL_NAMES)\n",
    "print(f\"Running Analysis...\\n\")\n",
    "print(f\"FEATURE\\tSMOOTHING METHOD\\tCLASSIFIER\")\n",
    "for trial in range(NUM_TRIALS):\n",
    "    for clf, clf_param_search_space, clf_name in classifiers:\n",
    "        for dataset, feature, smoothing_method in datasets:\n",
    "            print(f\"{feature}\\t{smoothing_method}\\t\\t{clf_name}\")\n",
    "\n",
    "            # train model\n",
    "            X = dataset.drop('emotionLabel', axis=1); y = dataset.emotionLabel\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "            grid = GridSearchCV(clf, clf_param_search_space, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", \n",
    "                                refit=True).fit(X_train, y_train)\n",
    "            \n",
    "            # record results in dataframe\n",
    "            test_set_accuracy_score = grid.score(X_test, y_test)\n",
    "            row = [[feature, smoothing_method, clf_name, test_set_accuracy_score]]\n",
    "            analysis_results_df = pd.concat([analysis_results_df, pd.DataFrame(row, columns=ANALYSIS_RESULTS_DF_COL_NAMES)])\n",
    "\n",
    "analysis_results_df = analysis_results_df.reset_index(drop=True)\n",
    "analysis_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "analysis_results_df.to_csv(\"seed_iv_analysis_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b434f8236d84722410e97d15e1caf3a9814e3cdf680c3ab017f12487e635d3b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
